---
title: "Homework Answer"
author: "Wenrui Hu"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework Answer}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# 2024.09.09

## Question

Use knitr to produce at least 3 examples. For each example,texts should mix with figures and/or tables. Better to have mathematical formulas.

## Answer

I will give three examples.Some will mix with figures ,tables and mathematical formulas.

## Example1
Firstly, I will reappear the regression analysis part in"Anskum Anscombe Quartet"
```{r}
anscombe <- data.frame(
  x1=c(10,8,13,9,11,14,6,4,12,7,5),
  x2=c(10,8,13,9,11,14,6,4,12,7,5),
  x3=c(10,8,13,9,11,14,6,4,12,7,5),
  x4=c(8,8,8,8,8,8,8,19,8,8,8),
  y1=c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68),
  y2=c(9.14,8.14,8.74,8.77,9.26,8.1,6.13,3.1,9.13,7.26,4.74),
  y3=c(7.46,6.77,12.74,7.11,7.81,8.84,6.08,5.39,8.15,6.42,5.73),
  y4=c(6.58,5.76,7.71,8.84,8.47,7.04,5.25,12.5,5.56,7.91,6.89)
)
# show results from four regression analyses
with(anscombe,print(summary(lm(y1~x1))))
with(anscombe,print(summary(lm(y2~x2))))
with(anscombe,print(summary(lm(y3~x3))))
with(anscombe,print(summary(lm(y4~x4))))

```

## Example2
Then，we will continue to use "Anskum Anscombe Quartet" for data analysis.
```{r}
  y1=c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68)
  y2=c(9.14,8.14,8.74,8.77,9.26,8.1,6.13,3.1,9.13,7.26,4.74)
  y3=c(7.46,6.77,12.74,7.11,7.81,8.84,6.08,5.39,8.15,6.42,5.73)
  y4=c(6.58,5.76,7.71,8.84,8.47,7.04,5.25,12.5,5.56,7.91,6.89)
  fs=c(y1,y2,y3,y4)
  mean(fs)
  median(fs)
  quantile(fs,probs=(0.25))
  quantile(fs,probs=(0.75))
  hist(fs)
```

## Example3
Finally,I will reappear chart in"Anskum Anscombe Quartet"
```{r}
anscombe <- data.frame(
  x1=c(10,8,13,9,11,14,6,4,12,7,5),
  x2=c(10,8,13,9,11,14,6,4,12,7,5),
  x3=c(10,8,13,9,11,14,6,4,12,7,5),
  x4=c(8,8,8,8,8,8,8,19,8,8,8),
  y1=c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68),
  y2=c(9.14,8.14,8.74,8.77,9.26,8.1,6.13,3.1,9.13,7.26,4.74),
  y3=c(7.46,6.77,12.74,7.11,7.81,8.84,6.08,5.39,8.15,6.42,5.73),
  y4=c(6.58,5.76,7.71,8.84,8.47,7.04,5.25,12.5,5.56,7.91,6.89)
)
pdf(file="fig_more_anscombe.pdf",width = 8.5,height = 8.5)
par(mfrow=c(2,2),mar=c(3,3,3,1))
with(anscombe,plot(x1,y1,xlim=c(2,20),ylim=c(2,14),pch=19,col="darkblue",cex=2,las=1))
title("Set 1")
with(anscombe,plot(x2,y2,xlim=c(2,20),ylim=c(2,14),pch=19,col="darkblue",cex=2,las=1))
title("Set 2")
with(anscombe,plot(x3,y3,xlim=c(2,20),ylim=c(2,14),pch=19,col="darkblue",cex=2,las=1))
title("Set 3")
with(anscombe,plot(x4,y4,xlim=c(2,20),ylim=c(2,14),pch=19,col="darkblue",cex=2,las=1))
title("Set 4")
dev.off()
```
# 2024.09.14

## Question1
The Rayleigh density [156, Ch. 18] is $$ f\left ( x \right ) =\frac{x}{\sigma ^{2} }  e^{\frac{-x ^{2}}{2\sigma ^{2}} } ,x\ge 0,\sigma> 0.$$ Develop an algorithm to generate random samples from a Rayleigh(σ) distribution. Generate Rayleigh(σ) samples for several choices of σ> 0 and check that the mode of the generated samples is close to the theoretical mode σ (check the histogram).

## Answer1
We can generate random samples from a Rayleigh(σ) distribution through the inverse transform method.Firstly, we can get $$F\left ( x \right ) = 1-e^{\frac{-x ^{2}}{2\sigma ^{2}} } ,\sigma> 0.$$,and we can deduce$$ F^{-1} \left ( u \right ) =\sqrt{-2 \sigma ^{2}\ln{(1-u)}  }  $$.Then we generate a uniformly distributed random number u and substitute it to get the Rayleigh(σ) distributed random number.In the end,graph the density histogram of the sample with several choices of σ> 0  density superimposed for comparison.

```{r}
n<-1000
u<-runif(n)
t=1
x<-(-2*t^2*log(1-u))^(1/2)#F(x)=1-e^(-(x^2)/2*t^2)
hist(x,prob=TRUE,main=expression(f(x)==x/(t^2)*exp(-(x^2)/2*t^2)))
y<-seq(0,10,.1)
lines(y,y/(t^2)*exp(-(y^2)/2*t^2))
```
```{r}
n<-1000
u<-runif(n)
t=0.5
x<-(-2*t^2*log(1-u))^(1/2)#F(x)=1-e^(-(x^2)/2*t^2)
hist(x,prob=TRUE,main=expression(f(x)==x/(t^2)*exp(-(x^2)/2*t^2)))
y<-seq(0,20,.1)
lines(y,y/(t^2)*exp(-(y^2)/2*t^2))
```
```{r}
n<-1000
u<-runif(n)
t=2
x<-(-2*t^2*log(1-u))^(1/2)#F(x)=1-e^(-(x^2)/2*t^2)
hist(x,prob=TRUE,main=expression(f(x)==x/(t^2)*exp(-(x^2)/2*t^2)))
y<-seq(0,10,.1)
lines(y,y/(t^2)*exp(-(y^2)/2*t^2))
```
Through these images, we can see that when σ is too big or too small, the effect is not very good, and when σ is moderate, the effect is better.

## Question2
Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have $N\left ( 0,1 \right )$  and $N\left ( 3,1 \right )$  distributions with mixing probabilities $p_{1}$ and $p_{2} = 1-p_{1}$ . Graph the histogram of the sample with density superimposed, for $p_{1} = 0.75$. Repeat with different values for $p_{1}$ and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of $p_{1}$ that produce bimodal mixtures.

## Answer2
First, we can generate random numbers from the $N\left ( 0,1 \right )$ and $N\left ( 3,1 \right )$ distributions through Box-Muller transformation and linear transformation of normal distribution.Then mix it according to different ratios to get the random number.



```{r}
n<-1000
p1<-0.75
p2<-1-p1
u<-runif(n)
v<-runif(n)
z1<-(-2*log(u))^(1/2)*cos(2*pi*v)
z2<-(-2*log(u))^(1/2)*sin(2*pi*v)+3
r<-sample(c(0,1),n,replace = TRUE,prob = c(p1,p2))
x<- r*z1+(1-r)*z2
hist(x,main=expression(p1==0.75))
```
```{r}
n<-1000
p1<-0.6
p2<-1-p1
u<-runif(n)
v<-runif(n)
z1<-(-2*log(u))^(1/2)*cos(2*pi*v)
z2<-(-2*log(u))^(1/2)*sin(2*pi*v)+3
r<-sample(c(0,1),n,replace = TRUE,prob = c(p1,p2))
x<- r*z1+(1-r)*z2
hist(x,main=expression(p1==0.6))
```

```{r}
n<-1000
p1<-0.5
p2<-1-p1
u<-runif(n)
v<-runif(n)
z1<-(-2*log(u))^(1/2)*cos(2*pi*v)
z2<-(-2*log(u))^(1/2)*sin(2*pi*v)+3
r<-sample(c(0,1),n,replace = TRUE,prob = c(p1,p2))
x<- r*z1+(1-r)*z2
hist(x,main=expression(p1==0.5))
```

```{r}
n<-1000
p1<-0.4
p2<-1-p1
u<-runif(n)
v<-runif(n)
z1<-(-2*log(u))^(1/2)*cos(2*pi*v)
z2<-(-2*log(u))^(1/2)*sin(2*pi*v)+3
r<-sample(c(0,1),n,replace = TRUE,prob = c(p1,p2))
x<- r*z1+(1-r)*z2
hist(x,main=expression(p1==0.4))
```

```{r}
n<-1000
p1<-0.25
p2<-1-p1
u<-runif(n)
v<-runif(n)
z1<-(-2*log(u))^(1/2)*cos(2*pi*v)
z2<-(-2*log(u))^(1/2)*sin(2*pi*v)+3
r<-sample(c(0,1),n,replace = TRUE,prob = c(p1,p2))
x<- r*z1+(1-r)*z2
hist(x,main=expression(p1==0.25))
```
We found that when p1 is around 0.5, the empirical distribution of the mixture appears to be bimodal.

##Question3

A compound Poisson process is a stochastic process$\left \{ X\left ( t \right ),t\ge 0  \right \}$ that can be represented as the random sum $$X\left ( t \right )=\sum_{i=1}^{N\left ( t \right )} Y_{i}  ,t\ge 0$$, where  $\left \{ N\left ( t \right ),t\ge 0  \right \}$ is a Poisson process and $Y_{1} ,Y_{2} ,...$are $iid$ and independent of $\left \{ N\left ( t \right ),t\ge 0  \right \}$.Write a program to simulate a compound Poisson($\lambda$)–Gamma process (Y has a Gamma distribution). Estimate the mean and the variance of $X\left ( 10 \right )$ for several choices of the parameters and compare with the theoretical values.Hint: Show that $E\left [ X\left ( t \right )  \right ] =\lambda t E\left [  Y_{1}\right ]$ and $Var\left (  X\left ( t \right ) \right ) =\lambda t E\left [  Y_{1}^{2}\right ]$

## Answer3

First, we need to generate some random numbers for the Poisson process.Then we generate the random numbers we need according to the composite process of Poisson(λ)-Gamma.Finally, the mean and variance of the obtained data are calculated and compared with the theoretical values, the theoretical value can be obtained from $E\left [ X\left ( t \right )  \right ] =\lambda t E\left [  Y_{1}\right ]$ and $Var\left (  X\left ( t \right ) \right ) =\lambda t E\left [  Y_{1}^{2}\right ]$

```{r}
n <- 10000
t <- 10 
lambda <- 1    
alpha <- 2     
gamma <- 3   
N_t <- rpois(n, lambda = lambda * t)
X_t <- numeric(n)  
for (i in 1:n)
{
  if (N_t[i] > 0) 
  {X_t[i] <- sum(rgamma(N_t[i], shape = alpha, rate = gamma))}  
   else 
  { X_t[i] <- 0 }
}
u <- mean(X_t)  
v <- var(X_t)    
u1 <- lambda * t * alpha / gamma
v1 <- lambda * t * (alpha / gamma^2)
cat("估计的 X(10) 均值: ", u,"\n")
cat("理论 X(10) 均值: ", u1, "\n")
cat("估计的 X(10) 方差: ", v, "\n")
cat("理论 X(10) 方差: ", v1, "\n")
```
```{r}
n <- 10000
t <- 10 
lambda <- 2    
alpha <- 3     
gamma <- 5      
N_t <- rpois(n, lambda = lambda * t)
X_t <- numeric(n)  
for (i in 1:n)
{
  if (N_t[i] > 0) 
  {X_t[i] <- sum(rgamma(N_t[i], shape = alpha, rate = gamma))}  
   else 
  { X_t[i] <- 0 }
}
u <- mean(X_t)  
v <- var(X_t)    
u1 <- lambda * t * alpha / gamma
v1 <- lambda * t * (alpha / gamma^2)
cat("估计的 X(10) 均值: ", u,"\n")
cat("理论 X(10) 均值: ", u1, "\n")
cat("估计的 X(10) 方差: ", v, "\n")
cat("理论 X(10) 方差: ", v1, "\n")
```
```{r}
n <- 10000
t <- 10 
lambda <- 5    
alpha <- 1    
gamma <- 5      
N_t <- rpois(n, lambda = lambda * t)
X_t <- numeric(n)  
for (i in 1:n)
{
  if (N_t[i] > 0) 
  {X_t[i] <- sum(rgamma(N_t[i], shape = alpha, rate = gamma))}  
   else 
  { X_t[i] <- 0 }
}
u <- mean(X_t)  
v <- var(X_t)    
u1 <- lambda * t * alpha / gamma
v1 <- lambda * t * (alpha / gamma^2)
cat("估计的 X(10) 均值: ", u,"\n")
cat("理论 X(10) 均值: ", u1, "\n")
cat("估计的 X(10) 方差: ", v, "\n")
cat("理论 X(10) 方差: ", v1, "\n")
```

# 2024.09.23

## Question1
Write a function to compute a Monte Carlo estimate of the Beta(3, 3) cdf,and use the function to estimate F(x) for x = 0.1, 0.2,..., 0.9. Compare the estimates with the values returned by the pbeta function in R.

## Answer1
Firstly, we know that the distribution density function of Beta (3,3) is $f\left ( x \right ) : 30x^{2} \left (  1-x\right ) ^{2}$,so its cumulative distribution function $F(x)$ is
$$
\int_{0}^{x}30t^{2} \left (  1-t\right ) ^{2} dt = xE\left [30T^{2} \left (  1-T\right ) ^{2} \right ],T\in U\left ( 0,x \right )  .
$$
Then we can use simple Monte Carlo estimator.
```{r}
 m <- 1e4; x<- c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)
 t <- runif(m, min=0, max=x)
 theta.hat <- mean(30*t^2*(1-t)^2) * x
 round(theta.hat,5)
 print(pbeta(x, shape1=3, shape2=3))
```

## Question2
The Rayleigh density [156, (18.76)] is 
$$
f(x)=\frac{x}{\sigma^2} e^{-x^2/(2\sigma^2)},\quad x\geq0, \sigma>0.
$$
Implement a function to generate samples from a Rayleigh(σ) distribution,using antithetic variables. What is the percent reduction in variance of $\frac{X+X^{\prime}}2$ compared with $\frac{X_{1}+X_{2}}2$for independent $X_{1},X_{2}$?

## Answer2
The problem is to estimate 
$$
F\left ( x \right ) =\int_{0}^{x} \frac{t}{\sigma^2} e^{-t^2/(2\sigma^2)}dt
$$
The simple estimator is
$$
 \hat{\theta}=\frac{1}{m}\sum_{j=1}^{m} x\frac{U_{j} }{\sigma^2} e^{-U_{j} ^2/(2\sigma^2)} ,U_{j}\sim U\left ( 0,x \right ) 
$$
The antithetic variable estimator is 
$$
 \hat{\theta}^{'} =\frac{1}{m}\sum_{j=1}^{m/2}\left (   x\frac{U_{j} }{\sigma^2} e^{-U_{j} ^2/(2\sigma^2)}+x\frac{\left (   x-U_{j}\right ) }{\sigma^2} e^{-\left (   x-U_{j}\right ) ^2/(2\sigma^2)}\right )   ,U_{j}\sim U\left ( 0,x \right )
$$
```{r}
f <- function(n, a) 
  {
  u <- runif(n) 
  x <- a * sqrt(-2 * log(u)) 
  }
g <- function(n, a) 
  {
  u <- runif(n/2) 
  x <- a * sqrt(-2 * log(u)) 
  xp <- a * sqrt(-2 * log(1 - u)) 
  return(c(x, xp))
  }
sd1 <- function(n, a) {
  x1 <- f(n, a)
  x2 <- f(n, a)
  x3 <- g(n, a)
  mean1 <- (x1 + x2) / 2
  mean2 <- (x3[1:(n/2)] + x3[(n/2 + 1):n]) / 2
  percent_reduction <- 100 * (var(mean1) - var(mean2)) / var(mean1)
}
 print(sd1(10000,1))
```

## Question3
Find two importance functions f1 and f2 that are supported on (1, ∞) and are ‘close’ to
$$g(x)=\frac{x^2}{\sqrt{2\pi}} e^{-x^2/2},\quad x>1.$$
Which of your two importance functions should produce the smaller variance in estimating
$$\int_1^\infty\frac{x^2}{\sqrt{2\pi}}e^{-x^2/2} dx$$
by importance sampling? Explain.

## Answer3
We can use $f_{1}(x)=e^{1-x}$ and $f_{2}(x)=\frac{1}{x^2}$ as importance functions.
```{r}
n=1e4;
est <- sd <- numeric(2)
g <- function(x) 
  {
  x^2*exp(-x^2/2)/sqrt(2*pi) * (x > 1)
  }
x<-rnorm(n)
f1g <- g(x)/exp(1-x)
u1 <- mean(f1g)
sd1 <- sd(f1g)
x <- rexp(n, 1)
f2g <- g(x)*x^2
u2 <- mean(f2g)
sd2 <- sd(f2g)
print(c(u1,u2,sd1,sd2))
```
From the data point of view, the result obtained by f1 function is better.

## Question4
Monte Carlo experiment.For $n = 10^{4}, 2 ×10^{4}, 4 × 10^{4}, 6 × 10^{4}, 8 × 10^{4}$, apply the fastsorting algorithm to randomly permuted numbers of 1, . . . , n.Calculate computation time averaged over 100 simulations,denoted by an.Regress an on tn := n log(n), and graphically show the results(scatter plot and regression line).

## Answer4
```{r}
quick_sort<-function(x)
  {
  num<-length(x)
      if(num==0||num==1){return(x)
        }else{
          t<-x[1]
          y<-x[-1]
          lower<-y[y<t]
          upper<-y[y>=t]
          return(c(quick_sort(lower),t,quick_sort(upper)))
        }
}
n<-c(1e4,2e4,4e4,6e4,8e4)
m<-100;y<-numeric(m);average<-numeric(5)
for(k in 1:5){
  for(j in 1:m){
    a<-sample(1:n[k])
    y[j]<-system.time(quick_sort(a))[1]
  }
  average[k]<-mean(y)
} 
average
z<-n*log(n)
lm(average~z)
plot(z,average)
lines(z,average)
```

# 2024.09.30

## Qusetion1
Estimate the 0.025, 0.05, 0.95, and 0.975 quantiles of the skewness $\sqrt{b_1}$ under normality by a Monte Carlo experiment. Compute the standard error of the estimates from (2.14) using the normal approximation for the density (with exact variance formula). Compare the estimated quantiles with the quantiles of the large sample approximation$\sqrt{b_1}\approx N(0,\frac{6}{n}).$

## Answer1
In fact, I haven't fully mastered this part of the content, so I made some requests for network information, and the code may not be perfect.
```{r}
skewness_values <- function(a,b){
   sqrt.b1<-numeric(m)
   c<-numeric(length(b))
   for (i in 1:m) {
    x <- mean(a[i,])
    y <- sd(a[i,])
    sqrt.b1[i] <- if((mean((a[i,] - x)^3) / y^3) >= 0){
    sqrt((mean((a[i,] - x)^3) / y^3))
    }else{
      sqrt.b1[i] <- NA
    }
   }
   sqrt.b1 <- na.omit(sqrt.b1)
   c <- quantile(sqrt.b1, probs = b)
   print(b)
   print(c)
}

normal_b<- function(n,b){
  t<-qnorm(b, mean = 0, sd = sqrt(6/m))
  print(t)
}

standard_errors <- function(b,x_values,m) {
    se <- sqrt((b*(1-b)) / (m * x_values^2)) 
    print(se)
}
n<-1000
m<-10000
b <- c(0.025, 0.05, 0.95, 0.975)
x_values <- qnorm(b)

a<- matrix(rnorm(m * n), nrow = m, ncol = n)

skewness_values(a,b)
normal_b(n,b)
standard_errors(b,x_values,m)

```
## Question2
Tests for association based on Pearson product moment correlation ρ, Spearman’s rank correlation coefficient ρs, or Kendall’s coefficient τ, are implemented in cor.test. Show (empirically) that the nonparametric tests based on ρs or τ are less powerful than the correlation test when the sampled distribution is bivariate normal. Find an example of an alternative (a bivariate
distribution (X, Y ) such that X and Y are dependent) such that at least one
of the nonparametric tests have better empirical power than the correlation
test against this alternative.

## Answer2
```{r}
n <- 1e3 
alpha <- 0.05
m <- 1e3

x <- rnorm(n, mean=1, sd=1) 
y <- rnorm(n, mean=0, sd=1)  

pearson_res <- cor.test(x, y, method = "pearson")
pearson_p_value <- pearson_res$p.value

spearman_res <- cor.test(x, y, method = "spearman")
spearman_p_value <- spearman_res$p.value

kendall_res <- cor.test(x, y, method = "kendall")
kendall_p_value <- kendall_res$p.value

cat("Pearson相关性检验:\n")
cat("相关系数:", pearson_res$estimate, "\n")
cat("p值:", pearson_p_value, "\n\n")

cat("Spearman相关性检验:\n")
cat("相关系数:", spearman_res$estimate, "\n")
cat("p值:", spearman_p_value, "\n\n")

cat("Kendall相关性检验:\n")
cat("相关系数:", kendall_res$estimate, "\n")
cat("p值:", kendall_p_value, "\n\n")
```
We find that nonparametric test based on ρs or τ is not as effective as correlation test when both samples are normally distributed.
```{r}
n <- 1e3 
alpha <- 0.05
m <- 1e3

x <-runif(n, -1, 1)
y <-rnorm(n, mean=0, sd=1) 

pearson_res <- cor.test(x, y, method = "pearson")
pearson_p_value <- pearson_res$p.value

spearman_res <- cor.test(x, y, method = "spearman")
spearman_p_value <- spearman_res$p.value

kendall_res <- cor.test(x, y, method = "kendall")
kendall_p_value <- kendall_res$p.value

cat("Pearson相关性检验:\n")
cat("相关系数:", pearson_res$estimate, "\n")
cat("p值:", pearson_p_value, "\n\n")

cat("Spearman相关性检验:\n")
cat("相关系数:", spearman_res$estimate, "\n")
cat("p值:", spearman_p_value, "\n\n")

cat("Kendall相关性检验:\n")
cat("相关系数:", kendall_res$estimate, "\n")
cat("p值:", kendall_p_value, "\n\n")
```
However, if one sample is uniformly distributed and the other is still normally distributed, nonparametric test will have better empirical efficacy than correlation test.

## Question3
If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. We want to know if the powers are different at 0.05 level.
1.What is the corresponding hypothesis test problem? What test should we use? 
2.Z-test, two-sample t-test,paired-t test or McNemar test? Why? 
3.Please provide the least necessary information for hypothes is testing.

##Answer3
1.H0:power1=power2 v.s H1:power1≠power2

2.because of the paired date,we can not use it;we can use paired-t test or McNemar test.But we can not use Z-test,because we do not know real variance.

3.we should knowthe  powers of two different methods and the mean and variance of their difference.


# 2024.10.14

## Question1

Of N = 1000 hypotheses, 950 are null and 50 are alternative.The p-value under any null hypothesis is uniformly distributed (use runif), and the p-value under any alternative hypothesis follows the beta distribution with parameter 0.1 and 1 (use rbeta). Obtain Bonferroni adjusted p-values and B-H adjusted
p-values. Calculate FWER, FDR, and TPR under nominal level $\alpha = 0.1$ for each of the two adjustment methods based on m = 10000 simulation replicates. You should output the 6 numbers (3 ) to a 3 × 2 table (column names: Bonferroni correction, B-H correction; row names: FWER, FDR, TPR).Comment the results.

## Answer1

```{r}
set.seed(456)
N<-1000
m0<-950
m<-10000
alpha<-0.1
FWER <- function(R, FP) {
  mean(FP > 0)
  }
FDR <- function(R, FP) {
  if (R == 0) return(0)
  mean(FP / R)
}
TPR <- function(R, TP) {
  if (R == 0) return(0)
  mean(TP / (N-m0)) 
}
y<-matrix(nrow = m,ncol = 6)
for (i in 1:m) {
  p<-c(runif(m0), rbeta(N-m0, 0.1, 1))
p.adj_bonferroni<-p.adjust(p,method = 'bonferroni')
p.adj_fdr<-p.adjust(p,method = 'fdr')
bonferroni_R <- sum(p.adj_bonferroni < alpha)
bonferroni_FP <- sum(p.adj_bonferroni[1:950] < alpha)
bonferroni_TP <- sum(p.adj_bonferroni[951:N] < alpha)
fdr_R <- sum(p.adj_fdr < alpha)
fdr_FP <- sum(p.adj_fdr[1:950] < alpha)
fdr_TP <- sum(p.adj_fdr[951:N] < alpha)
y[i,1]<-FWER(bonferroni_R,bonferroni_FP)
y[i,2]<-FDR(bonferroni_R,bonferroni_FP)
y[i,3]<-TPR(bonferroni_R,bonferroni_TP)
y[i,4]<-FWER(fdr_R,fdr_FP)
y[i,5]<-FDR(fdr_R,fdr_FP)
y[i,6]<-TPR(fdr_R,fdr_TP)
}
bonferroni_FWER<-mean(y[,1])
bonferroni_FDR<-mean(y[,2])
bonferroni_TPR<-mean(y[,3])
fdr_FWER<-mean(y[,4])
fdr_FDR<-mean(y[,5])
fdr_TPR<-mean(y[,6])
output_table <- data.frame(
  Metric = c("FWER", "FDR", "TPR"),
  Bonferroni = c(bonferroni_FWER, bonferroni_FDR,bonferroni_TPR),
  BH = c(fdr_FWER, fdr_TPR, fdr_TPR)
)

print(output_table)
```

In fact, I got a very strange result. There are great differences between Bonferroni method and B-H theory in FWER, FDR and TPR. Only TPR got similar results. Moreover, the results obtained by Bonferroni adjustment are less than those obtained by B-H adjustment.

## Question2

Refer to the air-conditioning data set aircondit provided in the boot package. The 12 observations are the times in hours between failures of airconditioning equipment [63, Example 1.1]:
                 3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487.
Assume that the times between failures follow an exponential model Exp(λ).Obtain the MLE of the hazard rate λ and use bootstrap to estimate the bias and standard error of the estimate

## Answer2
The title has explained that the sample is obtained from the exponential distribution exp (λ), so we can use the parameterized bootstrap method.First of all, we know the MLE of λ is $\frac{1}{\overline{x}}$.Then, we can estimate the bias and standard error of the estimate by parametric bootstrap method.

```{r}
x=c(3,5,7,18,43,85,91,98,100,130,230,487)
library(boot)
#定义参数分布
gen_function <- function(x,mle) {
    data <- rexp(length(x),mle)
    return(data)}
#定义计算统计量的函数
theta_star_function <- function(x,i) 1/mean(x[i])
B <- boot(data =x, sim = "parametric", ran.gen =  gen_function, mle = 1/mean(x), statistic = theta_star_function, R=1000)
round(c(original=B$t0,bias=mean(B$t)-B$t0,se=sd(B$t)),5)

```

## Question3

Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures 1/λ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

## Answer3

Let $\hat\theta=1/\hat\lambda=\overline x$.We need to compute 95% bootstrap confidence intervals for the mean time between failures 1/λ by the standard normal, basic, percentile, and BCa methods.So we must go back to parametric bootstrap method.

```{r}
set.seed(123)
library(boot)
x=c(3,5,7,18,43,85,91,98,100,130,230,487)
mle = mean(x)
#定义统计量计算的函数
theta_star_function <- function(x,i) mean(x[i])
#执行自助法
B <- boot(data = x, statistic = theta_star_function, R=1000)
#计算置信区间
ci_norm <- boot.ci(B, type = "norm")
ci_basic <- boot.ci(B, type = "basic")
ci_perc <- boot.ci(B, type = "perc")
ci_bca <- boot.ci(B, type = "bca")
cat("Standard Normal Method CI:\n", ci_norm$normal[2:3], "\n","Basic Method CI:\n", ci_basic$basic[4:5], "\n",
"Percentile Method CI:\n", ci_perc$percent[4:5], "\n","BCa Method CI:\n", ci_bca$bca[4:5], "\n")
```
Observing the results, we see that they all contain truth values $\overline x=108$.
Then calculate their interval length.
```{r}
ci_norm_lenth=ci_norm$normal[3]-ci_norm$normal[2]
ci_basic_lenth=ci_basic$basic[5]-ci_basic$basic[4]
ci_perc_lenth=ci_perc$percent[5]-ci_perc$percent[4]
ci_bca_lenth=ci_bca$bca[5]-ci_bca$bca[4]
cat(ci_norm_lenth,ci_basic_lenth,ci_perc_lenth,ci_bca_lenth)
```
We see that percentile, and BCa methods get the best results.
Finally, we repeated the experiment for many times to compare the correct rate of confidence intervals obtained by standard normal, basic, Percentile and BCA methods containing true values.

```{r}
set.seed(456)
library(boot)
x=c(3,5,7,18,43,85,91,98,100,130,230,487)
mle = mean(x)
m=1000
ci.norm<-ci.basic<-ci.perc<-ci.bca<-matrix(NA,m,2)
for(i in 1:m){
  B <- boot(data = x, statistic = theta_star_function, R=1000)#执行自助法
  ci <- boot.ci(B,type=c("norm","basic","perc","bca"))
  ci.norm[i,]<-ci$norm[2:3];ci.basic[i,]<-ci$basic[4:5]
  ci.perc[i,]<-ci$percent[4:5];ci.bca[i,]<-ci$bca[4:5]
}
cat('norm =',mean(ci.norm[,1]<=mle & ci.norm[,2]>=mle),
    'basic =',mean(ci.basic[,1]<=mle & ci.basic[,2]>=mle),
    'perc =',mean(ci.perc[,1]<=mle & ci.perc[,2]>=mle),
    'BCa =',mean(ci.bca[,1]<=mle & ci.bca[,2]>=mle))
```
We find that their correct rate is 1, and there is no error at all. This is not a good estimation result, it may be improved by other methods.

# 2024.10.21

## Question1
7.8 Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\widehat{\theta }$.

## Answer1
  First, we find the function theta of sample estimation parameters.Then, the estimated value $\widehat{\theta }$is calculated by using the sample data.Finally, the knife cutting method is used to calculate bias and standard error of $\widehat{\theta }$.
  
  Results and code implementation are as follows.
```{r}
library(bootstrap)
n <- nrow(scor)
theta <- function(x,i){
  val=eigen(cov(x[i,]))$values
  return(val[1]/sum(val))
}
theta.hat <- theta(scor,1:n)
theta.jack <- numeric(n)
for(i in 1:n){
  theta.jack[i] <- theta(scor,(1:n)[-i])
}
bias.jack <- (n-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((n-1)*mean((theta.jack-theta.hat)^2))
round(c(thetahat=theta.hat,bias.jack=bias.jack,
se.jack=se.jack),3)
```

## Question2
7.10 In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the Log-Log model with a cubic polynomial model. Which of the four models is selected by the cross validation procedure? Which model is selected according to maximum adjusted $R^{2}$?

## Answer2
Firstly, we calculate the residuals of the four models.
```{r}

library(DAAG);attach(ironslag)
n <- length(magnetic)
e1 <-e2 <-e3 <-e4 <-numeric(n)
for (k in 1:n){
  y <- magnetic[-k]
  x <- chemical[-k]
  
  J1 <- lm(y~x)
  yhat1 <- J1$coef[1]+J1$coef[2]*chemical[k]
  e1[k] <-magnetic[k] -yhat1
  
  J2 <- lm(y~x+I(x^2))
  yhat2 <- J2$coef[1]+J2$coef[2]*chemical[k]+J2$coef[3]*chemical[k]^2
  e2[k] <-magnetic[k] -yhat2
  
  J3 <- lm(log(y)~x)
  logyhat3 <- J3$coef[1]+J3$coef[2]*chemical[k]
  yhat3 <-exp(logyhat3)
  e3[k] <-magnetic[k] -yhat3
  
  J4 <- lm(y~x+I(x^2)+I(x^3))
  yhat4 <- J4$coef[1]+J4$coef[2]*chemical[k]+J4$coef[3]*chemical[k]^2+J4$coef[4]*chemical[k]^3
  e4[k] <-magnetic[k] -yhat4
}
c(mean(e1^2),mean(e2^2),mean(e3^2),mean(e4^2))
```
According to the residuals,we can see Model 2 would be the best fit for the data.
```{r}
J2
```
The fitted regression equation for model 2 is 
$$\hat{Y} =24.493-1.393X+0.055X^{2}$$
Then we can study maximum adjusted $R^{2}$ of the four models.
```{r}
r1=summary(J1)$adj.r.squared
r2=summary(J2)$adj.r.squared
r3=summary(J3)$adj.r.squared
r4=summary(J4)$adj.r.squared
c(r1,r2,r3,r4)
```
According to the maximum adjusted $R^{2}$,we can see Model 4 would be the best fit for the data.
```{r}
J4
```
The fitted regression equation for model 4 is
$$ \hat{Y} =2.764+2.908X-0.161X^{2}+0.003X^{3} $$

## Question3
8.1 Implement the two-sample Cram´er-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples 8.1 and 8.2.

## Answer3
  Similar to the expression in Example 8.2, we only need to construct the function cm.test to calculate new integrated squared distance between the distributions.
  
  Results and code implementation are as follows.
```{r}
attach(chickwts)
x <- sort(as.vector(weight[feed == "soybean"]))
y <- sort(as.vector(weight[feed == "linseed"]))
detach(chickwts)
set.seed(123)
R <- 999
z <- c(x,y)
K <- 1:26
D <- numeric(R)
cm.test <- function(x,y){
    Fn=ecdf(x)
    Gm=ecdf(y)
    n <- length(x)
    m <- length(y)
    a <-numeric(n)
    b <-numeric(m)
    for (i in 1:n) {
        a[i]=(Fn(x[i])-Gm(x[i]))^2
    }
    for (i in 1:m) {
        b[i]=(Fn(y[i])-Gm(y[i]))^2
    }
    s=sum(a)+sum(b)
    W2=s*m*n/(m+n)^2
    return(W2)
}
D0 <- cm.test(x,y)
for (i in 1:R) {
    k <- sample(K, size = 14,replace = FALSE)
    x1 <- z[k]
    y1 <- z[-k] #complement of x1
    D[i] <- cm.test(x1, y1)
}
p <- mean(c(D0, D)>= D0)
p
```
The approximate p-value 0.411 does not support the alternative hypothesis that distributions differ.

## Question4
8.2 Implement the bivariate Spearman rank correlation test for independence [255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples.

## Answer4
  With the function cor with method = "spearman", we can easily get the Spearman rank correlation test statistic.Then use the permutation test on it.
  Then we can get the achieved significance level of the permutation test.
```{r}
set.seed(123)
n <- 100
x <- rnorm(n)
y <- rnorm(n)
R <- 999 #number of replicate
z <- c(x,y)#pooled sample
K <- 1:(n+n)
reps <- numeric(R)# storage for replicate
cor0 <- cor(x,y,method="spearman")
for(i in 1:R){
  k <- sample(K,size=n,replace=FALSE)
  x1 <- z[k]
  y1 <- z[-k] #complement of x1
  reps[i] <- cor(x1,y1,method="spearman")
}
p <- mean(c(cor0,reps)>=cor0)
p
```
On the same sample, we can get the p-value reported by cor.test
```{r}
cor.test(x,y)
```
Compared with the two,we found that the P values of the two are similar,  notbut close enough.

# 2024.10.28

## Question1
Use the Metropolis-Hastings sampler to generate random variables from a standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution (see qcauchy or qt with df=1). Recall that a Cauchy(θ, η) distribution has density function $$f(x)=\frac{1}{\theta\pi(1+[(x-\eta)/\theta]^2)},\quad-\infty<x<\infty, \theta>0.$$
The standard Cauchy has the Cauchy(θ = 1, η = 0) density. (Note that the standard Cauchy density is equal to the Student t density with one degree of freedom.)

## Answer1

In order to generate random variables from the standard Cauchy distribution using the Metropolis-Hastings sampler, I take the student t distribution as the proposed function, and each $X_{n}$ will be generated from the t distribution with the absolute value of $X_{n}$. The process and the results obtained are as follows.
```{r}
library("coda")
f <- function(x) {
  return(1 / (pi * (1 + x^2)))
}
M_H <- function(N, initial_value) {
  x <- numeric(N)
  x[1] <- rt(1, df = 1)
  for (i in 2:N) {
    proposal <-rt(1, df = abs(x[i - 1]))
    alpha <- f(proposal) *dt(proposal,abs(x[i - 1]))/ f(x[i - 1])*dt(x[i - 1],abs(proposal))
    u <- runif(1)
    if (u<alpha) {
      x[i] <- proposal
    } 
       else {
      x[i] <- x[i - 1]
    }
  }
  return(x)
}
set.seed(123)
N <- 10000
initial_value <- 0
y<-M_H(N, initial_value)
x <- numeric(N-1000)
x<-y[1001:10000]
a1<-a2 <-numeric(10)
a1<-quantile(x,probs=seq(0.1,by=0.1))
a2<-qt(seq(0.1,by=0.1),1)
decile_comparison <- data.frame(
  Decile = seq(0.1, 1.0, by = 0.1),
  Observation_Deciles = a1,
  Theoretical_Deciles = a2
)
print(decile_comparison)
```

We compare the decimals of the generated observations with those of the standard Cauchy distribution, unfortunately, the obtained values are not very close. I guess this is because the degree of freedom of the proposed function I use takes advantage of the absolute value and weakens the good properties of the theory.

## Question2

This example appears in [40]. Consider the bivariate density
$$f(x,y)\propto\binom nxy^{x+a-1}(1-y)^{n-x+b-1},\quad x=0,1,\ldots,n, 0\leq y\leq1.$$.
It can be shown (see e.g. [23]) that for fixed a, b, n, the conditional distributions are $\mathrm{Binomial}(n,y)$ and $\mathrm{Beta}(x+a,n-x+b)$. Use the Gibbs sampler to generate a chain with target joint density $f(x,y)$.

## Answer2

Use the Gibbs sampler,we can easily generate a chain.The process and the results obtained are as follows.

```{r}
library(coda)
a <- 1
b <- 5
n <- 10
N <- 10000  
x <- 1
y <- 0
samples <- matrix(0, nrow = N, ncol = 2)
  
  for (i in 1:N) {
    x <- rbinom(1, n, y)
    y <- rbeta(1, x + a, n - x + b)
    samples[i, ] <- c(x, y)
  }
```

## Question3

For each of the above exercise, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\widehat{a}  < 1.2$.

## Answer3

In Question1:
```{r}
k <- 10 
chains <- lapply(1:k, function(i) M_H(N, initial_value))
mcmc_chains <- mcmc.list(lapply(chains, as.mcmc))
gelman_diag <- gelman.diag(mcmc_chains)
print(gelman_diag)
```
We can see that  $\widehat{R}$ is less than 1.2.It can be considered that Markov chains converge.

In Question2:
```{r}
k <- 10
chains <- vector("list", k)
for (j in 1:k) {
  x <- 0
  y <- 0.5
  samples <- matrix(0, nrow = N, ncol = 2)
  
  for (i in 1:N) {
    x <- rbinom(1, n, y)
    y <- rbeta(1, x + a, n - x + b)
    samples[i, ] <- c(x, y)
  }
  chains[[j]] <- as.mcmc(samples)
}
gelman_diag <- gelman.diag(mcmc.list(chains))
print(gelman_diag)
```
We can see that all $\widehat{R}$ are less than 1.1.It can be considered that Markov chains converge

# 2024.11.04

## Question1

Exercises 11.3  (a) Write a function to compute the $k^{th}$ term in
$$\sum_{k=0}^\infty\frac{(-1)^k}{k! 2^k}\frac{\left\|a\right\|^{2k+2}}{(2k+1)(2k+2)}\frac{\Gamma\left(\frac{d+1}{2}\right)\Gamma\left(k+\frac{3}{2}\right)}{\Gamma\left(k+\frac{d}{2}+1\right)},$$
where d ≥ 1 is an integer, a is a vector in $R^d$, and $\left \| \cdot  \right \|$denotes the Euclidean
norm. Perform the arithmetic so that the coefficients can be computed for
(almost) arbitrarily large k and d. (This sum converges for all $a\in R^{d}$)
(b) Modify the function so that it computes and returns the sum.
(c) Evaluate the sum when a = $(1, 2)^T$ .

## Answer1
(a)Let's assume 
$$a_k = \frac{(-1)^k}{k! 2^k}\frac{\left\|a\right\|^{2k+2}}{(2k+1)(2k+2)}\frac{\Gamma\left(\frac{d+1}{2}\right)\Gamma\left(k+\frac{3}{2}\right)}{\Gamma\left(k+\frac{d}{2}+1\right)}$$
$$c_k=\frac{1}{k! 2^k}\frac{\left\|a\right\|^{2k+2}}{(2k+1)(2k+2)}=exp\left \{  \left ( k+1 \right ) \log{\left \| a \right \| ^2} -klog2-log\left ( 2k+1 \right ) -log\left ( 2k+2 \right )-\sum_{i=1}^{k}log\left ( i \right ) \right \} $$
$$d_k=\frac{\Gamma\left(\frac{d+1}{2}\right)\Gamma\left(k+\frac{3}{2}\right)}{\Gamma\left(k+\frac{d}{2}+1\right)}=exp\left \{ log\Gamma\left(\frac{d+1}{2}\right)+log\Gamma\left({k+\frac{3}{2} }\right)-log\Gamma\left({k+\frac{d}{2}+1 }\right) \right \}$$
$$a_k=(-1)^k*c_k*d_k$$
We can use function a_k to compute the $k^{th}$ term.
```{r}
c_k <- function(k,a){
  l <- sum(a^2)
  s <- seq(1,k,1)
  r <- sum(log(s))
  m <- (k+1)*log(l)-k*log(2)-log(2*k+1)-log(2*k+2)-r
  return(exp(m))
}
d_k <- function(k,a){
   d <- length(a)
   n <- lgamma((d+1)/2)+lgamma(k+3/2)-lgamma(k+d/2+1)
   return(exp(n))
}
a_k <- function(k,a){
  t <- (-1)^(k)*c_k(k,a)*d_k(k,a)
  return(t)
}
```

(b)We only need to sum each function a_k to get the result.
```{r}
S_n <- function(n,a){
t <- numeric(n)
for (i in 1:n) {
    t[i] <- a_k(i,a)
}
d <- length(a)
s0 <- sum(a^2)*gamma((d+1)/2)*gamma(3/2)/(2*gamma((d+2)/2))
ss <- sum(t)+s0
return(ss)
}
```

(c)We can evaluate the sum when a = $(1, 2)^T$ and n = 100.
```{r}
a <- c(1,2)
n <- 100
S_n(n,a)
```



## Question2
Exercises 11.5 Write a function to solve the equation
$$\frac{2\Gamma(\frac{k}{2})}{\sqrt{\pi(k-1)}\Gamma(\frac{k-1}{2})}\int_{0}^{c_{k-1}}\left(1+\frac{u^{2}}{k-1}\right)^{-k/2}du\\=\frac{2\Gamma(\frac{k+1}{2})}{\sqrt{\pi k}\Gamma(\frac{k}{2})}\int_{0}^{c_{k}}\left(1+\frac{u^{2}}{k}\right)^{-(k+1)/2}du$$
for a, where
$$c_k=\sqrt{\frac{a^2k}{k+1-a^2}}.$$
Compare the solutions with the points A(k) in Exercise 11.4.

## Answer2
First of all, we can equivalent the equation to:
$$\sqrt{k}\Gamma^2(\frac{k}{2})\int_{0}^{c_{k-1}}\left(1+\frac{u^{2}}{k-1}\right)^{-k/2}du-\sqrt{(k-1)}\Gamma(\frac{k+1}{2})\Gamma(\frac{k-1}{2})\int_{0}^{c_{k}}\left(1+\frac{u^{2}}{k}\right)^{-(k+1)/2}du=0$$

```{r}
m = function (k) {g.integral = function(u, n) {
  (1 + u^2/(n-1))^(-n/2)
}
l = function (n, a) {
  sqrt(a^2 * n / (n + 1 - a^2))
}
g = function (n, a) {
    q.integral = function (u) {g.integral(u, n)}
    c = l(n - 1, a)
    2/sqrt(pi*(n-1)) * exp(lgamma(n/2)-lgamma((n-1)/2)) * integrate(q.integral, lower = 0, upper = c)$value}
  f = function (a) {
    t1 = g(k, a)
    t2 = g(k + 1, a)
    return (t1 - t2)
  }
  
  tol = 1e-2
  if (f(tol) < 0 && f(sqrt(k) - tol) > 0 || f(tol) > 0 && f(sqrt(k) - tol) < 0) {
    r = uniroot(f, interval = c(tol, sqrt(k)-tol))$root
  } else {
    r = NA
  }
  return(r)
}

s= sapply(c(5:20), function (k) {
  m(k)
})
s
```
We can get the above results by using the existing code in the online community.



## Question3

Suppose $T_i,\ldots,T_n$ are i.i.d. samples drawn from the exponential distribution with expectation λ. Those values greater than $\tau$ are not observed due to right censorship, so that the observed values are $$Y_i=T_iI(T_i\leq\tau)+\tau I(T_i>\tau),i=1,\ldots,n.$$
Suppose $\tau=1$ and the observed $Y_i$ values are as follows:
            0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85
Use the E-M algorithm to estimate $\lambda$, compare your result with the observed data MLE (note: $Y_i$ follows a mixture distribution).

## Answer3
Complete data likelihood:
$$l\left ( \lambda \mid Y_i \right ) =n\log{\lambda } -\lambda \left ( \sum_{i=1}^{n} T_i \right )$$
$$l\left ( \lambda \mid Y_i \right ) =n\log{\lambda } -\lambda \left ( \sum_{i=1}^{n} Y_i+\sum_{i=1}^{n} (T_i-1) I\left ( T_i >1 \right )  \right )$$
E-step:

$$E\left ( T_i \mid T_{i} > 1 \right )=1+\frac{1}{\lambda_0 }$$
$$E_{\hat{\lambda } }\left [ l\left ( \lambda \mid Y_i \right ) \right ] =n\log{\lambda } -\lambda \left ( \sum_{i=1}^{n} Y_i+ n_1 \frac{1}{\lambda_0 }   \right )$$
where  $n_1=\sum_{i=1}^{n} Y_i I\left ( Y_i = 1 \right)$ 

M-step:

$$\hat{\lambda_1 } =\frac{1}{\overline{Y} -\frac{n_1}{n\lambda _0} } $$
```{r}
Y <- c(0.54, 0.48, 0.33, 0.43, 1.00, 1.00, 0.91, 1.00, 0.21, 0.85)
tau <- 1  
lambda <- 1  
N <- 1000
tol <- 1e-6
for (iter in 1:N) {
  m <- Y[Y < tau]
  n <- Y[Y >= tau]
  E <- tau + 1 / lambda  
  lambda_new <- length(Y) / (sum(m) + length(n) * E)
  if (abs(lambda_new - lambda) < tol) {
    lambda <- lambda_new
    break
  }
  lambda <- lambda_new
}
lambda
```
We got a good estimate of $\lambda$.

# 2024.11.11

## Question1
Use the simplex algorithm to solve the following problem.
Minimize $4x + 2y + 9z$ subject to
$$2x + y + z ≤ 2$$
$$x − y + 3z ≤ 3$$
$$x ≥ 0, y ≥ 0, z ≥ 0$$

## Answer1
```{r}
library(boot) #for simplex function
A1 <- rbind(c(2,1,1),c(1,-1,3))
b1 <- c(2,3)
a <- c(4,2,9)
simplex(a=a,A1=A1,b1=b1,maxi = FALSE)
```
We can see the equation gets the minimum value of 0 when x=y=z=0.

## Question2
Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:
```{r}

formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
```


## Answer2
```{r}
data(mtcars)
formulas <- list(
  mpg ~ disp,
  mpg ~ I(1 / disp),
  mpg ~ disp + wt,
  mpg ~ I(1 / disp) + wt
)
for (i in formulas) {
  y <- lm(i,mtcars)
  print(summary(y))
}
models <- lapply(formulas, function(i) lm(i, data = mtcars))
lapply(models, summary)

```
```{r}

```


## Question3
Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply().Can you do it without an anonymous function?
```{r}
bootstraps <- lapply(1:10, function(i) {
rows <- sample(1:nrow(mtcars), rep = TRUE)
mtcars[rows, ]
})

```


## Answer3

```{r}
data(mtcars)
f <- function() {
  i <- sample(1:nrow(mtcars), replace = TRUE)
  mtcars[i, ]
}
bootstraps <- lapply(1:10, function(i) f())
m <- function(data) {
  lm(mpg ~ disp, data = data)
}
x <- list()
for (i in 1:length(bootstraps)) {
  x[[i]] <- m(bootstraps[[i]])
}
y <- lapply(bootstraps, m)
lapply(x, summary)
lapply(y, summary)
```

## Question4
For each model in the previous two exercises, extract R2 using the function below.
```{r}
rsq <- function(mod) summary(mod)$r.squared
```


## Answer4

```{r}
rsq <- function(mod) summary(mod)$r.squared
x1 <- sapply(x, rsq)
y1 <- sapply(y, rsq)
print(x1)
print(y1)
```
## Quesyion5
The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.
```{r}
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
```
Extra challenge: get rid of the anonymous function by using
[[ directly.

## Answer5
```{r}
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)
p <- sapply(trials, `[[`,"p.value")
print(p)
```

## Quesyion6
Implement a combination of Map() and vapply() to create an lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

## Answer6
```{r}
f <- function(FUN, ..., simplify = TRUE) {
  results <- Map(FUN, ...)
  vapply(results, identity, FUN.VALUE = simplify)
}
```

## Quesyion7
Make a faster version of chisq.test() that only computes the chi-square test statistic when the input is two numeric vectors with no missing values. You can try simplifying chisq.test() or by coding from the mathematical definition (http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test).
## Answer7
```{r}
f <- function(x, y) {
  if (!is.numeric(x) || !is.numeric(y) || anyNA(x) || anyNA(y)) {
    break
  }
  m <- table(x, y)
  n <- outer(rowSums(m), colSums(m)) / sum(m)
  z <- sum((m - n)^2 / n)
  return(z)
}
```

## Quesyion8
Can you make a faster version of table() for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test?

## Answer8
```{r}
fast_table <- function(x, y) {
  if (!is.integer(x) || !is.integer(y) || anyNA(x) || anyNA(y)) {
    break
  }
  m <- matrix(0, nrow = length(sort(unique(x))), ncol = length(sort(unique(y))),
                         dimnames = list(sort(unique(x)), sort(unique(y))))
  
  for (i in seq_along(x)) {
    m[as.character(x[i]), as.character(y[i])] <- m[as.character(x[i]), as.character(y[i])] + 1
  }
  
  return(m)
}
```

# 2024.11.18

## Question

This example appears in [40]. Consider the bivariate density

$$f(x,y)\propto\binom{n}{x}y^{x+a-1}(1-y)^{n-x+b-1},\quad x=0,1,\ldots,n,\:0\leq y\leq1.$$

It can be shown (see e.g. [23]) that for fixed $a,b,n$, the conditional distributions are Binomial$(n,y)$ and Beta$(x+a,n-x+b).$ Use the Gibbs sampler to generate a chain with target joint density $f(x,y).$

## Answer1
```{r}
n <- 10  
a <- 1   
b <- 2   
iterations <- 10000 

x <- numeric(iterations)
y <- numeric(iterations)
x[1] <- sample(0:n, 1) 
y[1] <- runif(1)

for (i in 2:iterations) {
  
  y[i] <- rbeta(1, shape1 = x[i-1] + a, shape2 = n - x[i-1] + b)
  
  x[i] <- rbinom(1, size = n, prob = y[i])
}

plot(x, y, main = "Gibbs Sampler", xlab = "x", ylab = "y", pch = 16, col = "green")
```

## Question2
Compare the corresponding generated random numbers with those by the R function you wrote using the function “qqplot”.

Campare the computation time of the two functions with the function “microbenchmark”.

Comments your results.

## Answer2
```{r}
library(microbenchmark)
n <- 10
a <- 1
b <- 2
iterations <- 10000

gibbs_x <- numeric(iterations)
gibbs_y <- numeric(iterations)
gibbs_x[1] <- sample(0:n, 1)
gibbs_y[1] <- runif(1)

for (i in 2:iterations) {
  gibbs_y[i] <- rbeta(1, shape1 = gibbs_x[i-1] + a, shape2 = n - gibbs_x[i-1] + b)
  gibbs_x[i] <- rbinom(1, size = n, prob = gibbs_y[i])
}

direct_y <- rbeta(iterations, shape1 = a, shape2 = n + b - a)
direct_x <- rbinom(iterations, size = n, prob = direct_y)

qqplot(gibbs_y, direct_y, main = "Gibbs采样与直接采样的y分布QQ-图", xlab = "Gibbs采样器", ylab = "直接采样")
abline(0, 1, col = "green")

qqplot(gibbs_x, direct_x, main = "Gibbs采样与直接采样的x分布QQ-图", xlab = "Gibbs采样器", ylab = "直接采样")
abline(0, 1, col = "green")

gibbs_time <- microbenchmark({
  gibbs_x <- numeric(iterations)
  gibbs_y <- numeric(iterations)
  gibbs_x[1] <- sample(0:n, 1)
  gibbs_y[1] <- runif(1)
  for (i in 2:iterations) {
    gibbs_y[i] <- rbeta(1, shape1 = gibbs_x[i-1] + a, shape2 = n - gibbs_x[i-1] + b)
    gibbs_x[i] <- rbinom(1, size = n, prob = gibbs_y[i])
  }
}, times = 10)

direct_time <- microbenchmark({
  direct_y <- rbeta(iterations, shape1 = a, shape2 = n + b - a)
  direct_x <- rbinom(iterations, size = n, prob = direct_y)
}, times = 10)

print(gibbs_time)
print(direct_time)
```
We can see the QQ-plot will show how closely the distribution of the output from the Gibbs sampler corresponds to that of the direct sampling. If the points are roughly aligned along the red line (y=x), this indicates that the distributions are similar.

Direct sampling is faster because it avoids the iterative process inherent in Gibbs sampling, which requires multiple steps for each iteration.

